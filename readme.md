ğŸ§  GPT from Scratch â€” Transformer Language Model
This project implements a Generatively Pretrained Transformer (GPT) from scratch, inspired by the seminal paper â€œAttention is All You Needâ€ and OpenAIâ€™s GPT-2/GPT-3 architecture.

It is a minimal, educational codebase that walks through the foundational ideas behind autoregressive language models and transformer architectures, focusing on clarity and full visibility into what happens under the hood.

ğŸš€ Whatâ€™s Inside
âœ… Tokenization and dataset preparation (Tiny Shakespeare)

âœ… Bigram language model as a baseline

âœ… Self-attention mechanism implemented from first principles

âœ… Positional encoding

âœ… Multi-head self-attention

âœ… Transformer blocks: feedforward, residuals, normalization

âœ… Training loop using PyTorch

âœ… Scaling and model regularization (dropout, layer norm)

âœ… Discussion on architectural tradeoffs and pretraining vs. fine-tuning

.
â”œâ”€â”€ manage-1.ipynb            # Entry point: training loop
â”œâ”€â”€ test.py            # Transformer architecture
â”œâ”€â”€ data.py             # Dataset preprocessing and loader
â”œâ”€â”€ config.py           # Model and training hyperparameters
â”œâ”€â”€ utils.py            # Helper functions
â””â”€â”€ README.md           # You're here


ğŸ™ Acknowledgements
This project is heavily inspired by the works of Andrej Karpathy and the open-source AI community. It is built as a self-contained educational resource to deepen understanding of how modern LLMs work.

ğŸ”— Key References
Attention is All You Need (Vaswani et al.)

GPT-3: Language Models are Few-Shot Learners (Brown et al.)

OpenAI ChatGPT Blog Post

nanoGPT â€” A scalable and modernized implementation

Lambda Labs â€” For cloud GPU compute used in training